{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87737861-0cba-441d-af3a-d8a582c172a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install langgraph openai ffmpeg-python requests python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e7cce6-2be6-41c8-8cd4-198c39996215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import json\n",
    "from langchain.tools import Tool\n",
    "from langchain.tools import StructuredTool\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.prebuilt import create_react_agent\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from openai import OpenAI\n",
    "\n",
    "# Load environment variables\n",
    "_ = load_dotenv()\n",
    "ELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")  # Set your ElevenLabs API key in the .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "88fc4827-2e86-4b63-bb53-4d24136f5fc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting audio from downloads/CleanVoice.mov to downloads/CleanVoice_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 16.0.0 (clang-1600.0.26.4)\n",
      "  configuration: --prefix=/usr/local/Cellar/ffmpeg/7.1_4 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "Input #0, mov,mp4,m4a,3gp,3g2,mj2, from 'downloads/CleanVoice.mov':\n",
      "  Metadata:\n",
      "    major_brand     : qt  \n",
      "    minor_version   : 0\n",
      "    compatible_brands: qt  \n",
      "    creation_time   : 2025-02-12T21:27:10.000000Z\n",
      "    com.apple.quicktime.make: Apple\n",
      "    com.apple.quicktime.model: MacBookPro16,1\n",
      "    com.apple.quicktime.software: macOS 14.6.1 (23G93)\n",
      "    com.apple.quicktime.creationdate: 2025-02-12T22:21:23+0100\n",
      "  Duration: 00:04:22.62, start: 0.000000, bitrate: 6678 kb/s\n",
      "  Stream #0:0[0x1](und): Video: h264 (Main) (avc1 / 0x31637661), yuv420p(tv, bt709, progressive), 3332x1634 [SAR 1:1 DAR 1666:817], 6540 kb/s, 59.97 fps, 60 tbr, 6k tbn (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-02-12T21:27:10.000000Z\n",
      "        handler_name    : Core Media Video\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : H.264\n",
      "  Stream #0:1[0x2](und): Audio: aac (LC) (mp4a / 0x6134706D), 48000 Hz, mono, fltp, 129 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-02-12T21:27:10.000000Z\n",
      "        handler_name    : Core Media Audio\n",
      "        vendor_id       : [0][0][0][0]\n",
      "Stream mapping:\n",
      "  Stream #0:1 -> #0:0 (aac (native) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'downloads/CleanVoice_audio.wav':\n",
      "  Metadata:\n",
      "    major_brand     : qt  \n",
      "    minor_version   : 0\n",
      "    compatible_brands: qt  \n",
      "    com.apple.quicktime.creationdate: 2025-02-12T22:21:23+0100\n",
      "    com.apple.quicktime.make: Apple\n",
      "    com.apple.quicktime.model: MacBookPro16,1\n",
      "    com.apple.quicktime.software: macOS 14.6.1 (23G93)\n",
      "    ISFT            : Lavf61.7.100\n",
      "  Stream #0:0(und): Audio: pcm_s16le ([1][0][0][0] / 0x0001), 48000 Hz, mono, s16, 768 kb/s (default)\n",
      "      Metadata:\n",
      "        creation_time   : 2025-02-12T21:27:10.000000Z\n",
      "        handler_name    : Core Media Audio\n",
      "        vendor_id       : [0][0][0][0]\n",
      "        encoder         : Lavc61.19.100 pcm_s16le\n",
      "[out#0/wav @ 0x7f93be805300] video:0KiB audio:24617KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.000309%\n",
      "size=   24617KiB time=00:04:22.57 bitrate= 768.0kbits/s speed= 864x    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Audio extraction successful: downloads/CleanVoice_audio.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeny/Library/Python/3.9/lib/python/site-packages/pydantic/main.py:390: UserWarning: Pydantic serializer warnings:\n",
      "  Expected `str` but got `float` with value `262.57000732421875` - serialized value may not be as expected\n",
      "  return self.__pydantic_serializer__.to_python(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Transcript passed to LLM for cleaning: Hi This video showcases an AI agent designed to produce a clean voiceover by replacing the original recorded voice with an AI generated one The agent is built on the LANGRAF framework and operates autonomously using six tools You can see the tools here ExtractAudioTool is using FFmpeg TranscribeAudioTool uses WhisperAI CleanTranscriptTool is using OpenAI's GPT 4 0 SOR language model AIVoiceTool uses LevelLabs where I have cloned my voice And the AIVoiceTool obviously generates the AI voice RemoveAudioVoiceTool is again utilizing FFmpeg And finally UnmergeTool which is using again FFmpeg Since it's a reasoning action agent it decides on its own the sequence of two executions in order to complete the task efficiently So when it comes to building the agent the steps go like this First I've decided on which tools to use and have created the tools definition Everything from the beginning is simply tools definition extract audio The tools are defined with a schema which is defining the arguments which the tool takes for the execution The input in essence of the function Then there is the registration with LANGRAF Then the next tool is TranscribeAudio CleanTranscript The next one is GenerateAIVoice with LevelLabs There is also a SlowDownAudio option because the AI generated one is very quick RemoveOldVoice MergeAudio and yeah Finally this is the tool list Then the agent creation Now let's execute it and see how it works I have a very short video recording which I will use for it You will be able to see which files have been created and how the process goes Let's start the execution The voice is already split The first transcript from WhisperAI is created The cleaned transcript by the LLM is also created Then there is the AI voice Then finally we have the cleaned original video This time the agent didn't produce the final merged file Let's see here how the execution looks like Successfully created cleaned audio and the final merging is not happening Let's try quickly again to see if it will merge it Right here it produced the final video Let's quickly play it It's quite fast We can slow it down a bit That's it The video you are watching right now was processed using this agent Check the links below to see the original version Thank you Bye\n",
      "‚úÖ Transcript passed to ElevenLabs: Hi This video showcases an AI agent designed to produce a clean voiceover by replacing the original recorded voice with an AI generated one The agent is built on the LANGRAF framework and operates autonomously using six tools You can see the tools here ExtractAudioTool is using FFmpeg TranscribeAudioTool uses WhisperAI CleanTranscriptTool is using OpenAI's GPT 4 0 SOR language model AIVoiceTool uses LevelLabs where I have cloned my voice And the AIVoiceTool obviously generates the AI voice RemoveAudioVoiceTool is again utilizing FFmpeg And finally UnmergeTool which is using again FFmpeg Since it's a reasoning action agent it decides on its own the sequence of two executions in order to complete the task efficiently So when it comes to building the agent the steps go like this First I've decided on which tools to use and have created the tools definition Everything from the beginning is simply tools definition extract audio The tools are defined with a schema which is defining the arguments which the tool takes for the execution The input in essence of the function Then there is the registration with LANGRAF Then the next tool is TranscribeAudio CleanTranscript The next one is GenerateAIVoice with LevelLabs There is also a SlowDownAudio option because the AI generated one is very quick RemoveOldVoice MergeAudio and ...... Finally this is the tool list Then the agent creation Now let's execute it and see how it works I have a very short video recording which I will use for it You will be able to see which files have been created and how the process goes Let's start the execution The voice is already split The first transcript from WhisperAI is created The cleaned transcript by the LLM is also created Then there is the AI voice Then finally we have the cleaned original video This time the agent didn't produce the final merged file Let's see here how the execution looks like Successfully created cleaned audio and the final merging is not happening Let's try quickly again to see if it will merge it Right here it produced the final video Let's quickly play it It's quite fast We can slow it down a bit That's it The video you are watching right now was processed using this agent Check the links below to see the original version Thank you Bye\n",
      "‚úÖ AI voice successfully generated: downloads/CleanVoice_AI_audio.wav\n",
      "‚úÖ Slowing down the audio!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ffmpeg version 7.1 Copyright (c) 2000-2024 the FFmpeg developers\n",
      "  built with Apple clang version 16.0.0 (clang-1600.0.26.4)\n",
      "  configuration: --prefix=/usr/local/Cellar/ffmpeg/7.1_4 --enable-shared --enable-pthreads --enable-version3 --cc=clang --host-cflags= --host-ldflags='-Wl,-ld_classic' --enable-ffplay --enable-gnutls --enable-gpl --enable-libaom --enable-libaribb24 --enable-libbluray --enable-libdav1d --enable-libharfbuzz --enable-libjxl --enable-libmp3lame --enable-libopus --enable-librav1e --enable-librist --enable-librubberband --enable-libsnappy --enable-libsrt --enable-libssh --enable-libsvtav1 --enable-libtesseract --enable-libtheora --enable-libvidstab --enable-libvmaf --enable-libvorbis --enable-libvpx --enable-libwebp --enable-libx264 --enable-libx265 --enable-libxml2 --enable-libxvid --enable-lzma --enable-libfontconfig --enable-libfreetype --enable-frei0r --enable-libass --enable-libopencore-amrnb --enable-libopencore-amrwb --enable-libopenjpeg --enable-libspeex --enable-libsoxr --enable-libzmq --enable-libzimg --disable-libjack --disable-indev=jack --enable-videotoolbox --enable-audiotoolbox\n",
      "  libavutil      59. 39.100 / 59. 39.100\n",
      "  libavcodec     61. 19.100 / 61. 19.100\n",
      "  libavformat    61.  7.100 / 61.  7.100\n",
      "  libavdevice    61.  3.100 / 61.  3.100\n",
      "  libavfilter    10.  4.100 / 10.  4.100\n",
      "  libswscale      8.  3.100 /  8.  3.100\n",
      "  libswresample   5.  3.100 /  5.  3.100\n",
      "  libpostproc    58.  3.100 / 58.  3.100\n",
      "[mp3 @ 0x7fc936104840] Estimating duration from bitrate, this may be inaccurate\n",
      "Input #0, mp3, from 'downloads/CleanVoice_AI_audio.wav':\n",
      "  Duration: 00:02:58.49, start: 0.000000, bitrate: 127 kb/s\n",
      "  Stream #0:0: Audio: mp3 (mp3float), 44100 Hz, mono, fltp, 128 kb/s\n",
      "Stream mapping:\n",
      "  Stream #0:0 -> #0:0 (mp3 (mp3float) -> pcm_s16le (native))\n",
      "Press [q] to stop, [?] for help\n",
      "Output #0, wav, to 'downloads/CleanVoice_AI_audio_temp.wav':\n",
      "  Metadata:\n",
      "    ISFT            : Lavf61.7.100\n",
      "  Stream #0:0: Audio: pcm_s16le ([1][0][0][0] / 0x0001), 44100 Hz, mono, s16, 705 kb/s\n",
      "      Metadata:\n",
      "        encoder         : Lavc61.19.100 pcm_s16le\n",
      "[out#0/wav @ 0x7fc93570f6c0] video:0KiB audio:17081KiB subtitle:0KiB other streams:0KiB global headers:0KiB muxing overhead: 0.000446%\n",
      "size=   17081KiB time=00:03:18.31 bitrate= 705.6kbits/s speed= 505x    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ AI voice successfully slowed down: downloads/CleanVoice_AI_audio.wav\n",
      "üßπ Removing all audio from downloads/CleanVoice.mov -> downloads/CleanVoice_cleaned.mov\n",
      "‚úÖ Successfully created cleaned video: downloads/CleanVoice_cleaned.mov\n",
      "üé¨ Merging (no timing adjust) video: downloads/CleanVoice_cleaned.mov + AI voice: downloads/CleanVoice_AI_audio.wav\n",
      "‚úÖ Successfully created final video: downloads/CleanVoice_final.mov\n"
     ]
    }
   ],
   "source": [
    "# First create the schema\n",
    "class ExtractAudioSchema(BaseModel):\n",
    "    video_path: str\n",
    "    output_audio_path: str\n",
    "\n",
    "\n",
    "def extract_audio(video_path: str, output_audio_path: str = None) -> str:\n",
    "    \"\"\"Extracts audio from a video file using FFmpeg and ensures it exists before returning.\"\"\"\n",
    "    \n",
    "    if output_audio_path is None:\n",
    "        base_name = os.path.splitext(os.path.basename(video_path))[0]\n",
    "        output_audio_path = os.path.join(\"downloads\", f\"{base_name}_audio.wav\")\n",
    "\n",
    "    print(f\"Extracting audio from {video_path} to {output_audio_path}\")\n",
    "\n",
    "    subprocess.run([\"ffmpeg\", \"-i\", video_path, \"-q:a\", \"0\", \"-map\", \"a\", output_audio_path], check=True)\n",
    "\n",
    "    # Wait for file to exist (retry for 5 seconds)\n",
    "    for _ in range(10):  # Check 10 times (every 0.5s)\n",
    "        if os.path.exists(output_audio_path) and os.path.getsize(output_audio_path) > 0:\n",
    "            print(f\"‚úÖ Audio extraction successful: {output_audio_path}\")\n",
    "            return output_audio_path\n",
    "        time.sleep(0.5)\n",
    "\n",
    "    raise FileNotFoundError(f\"Audio extraction failed: {output_audio_path} was not created or is empty.\")\n",
    "\n",
    "# Register extraction tool\n",
    "extract_audio_tool = StructuredTool(\n",
    "    name=\"ExtractAudio\",\n",
    "    func=extract_audio,\n",
    "    description=\"Extracts audio from a video file using FFmpeg. Ensures the extracted file exists before returning.\",\n",
    "    args_schema=ExtractAudioSchema\n",
    ")\n",
    "\n",
    "# Initialize OpenAI Client\n",
    "client = OpenAI(api_key=os.getenv(\"OPENAI_API_KEY\"))\n",
    "\n",
    "# Define Schema\n",
    "class TranscribeAudioSchema(BaseModel):\n",
    "    audio_path: str\n",
    "\n",
    "# Transcribe Audio Tool using OpenAI Whisper API: https://platform.openai.com/docs/guides/speech-to-text\n",
    "# File Size Limit: 25 MB (for a single request)\n",
    "# Duration Limit: ~30 minutes (varies depending on bitrate and file compression)\n",
    "# If your audio file exceeds 25 MB, you‚Äôll need to chunk it into smaller segments before processing!    \n",
    "def transcribe_audio(audio_path: str) -> str:\n",
    "    \"\"\"Transcribes audio using OpenAI Whisper API and saves the transcript with timestamps in a new file.\"\"\"\n",
    "\n",
    "    # Ensure file exists\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "    # Ensure file format is correct\n",
    "    if not audio_path.endswith(\".wav\"):\n",
    "        raise ValueError(f\"Expected a WAV file, but got: {audio_path}\")\n",
    "\n",
    "    with open(audio_path, \"rb\") as audio_file:\n",
    "        # Call OpenAI Whisper API with word-level timestamps\n",
    "        transcription = client.audio.transcriptions.create(\n",
    "            file=audio_file,\n",
    "            model=\"whisper-1\",\n",
    "            response_format=\"verbose_json\",\n",
    "            timestamp_granularities=[\"word\"]  # Supports [word, segment] granularity\n",
    "        )\n",
    "\n",
    "    # Create a new file for the transcript (without replacing anything)\n",
    "    new_filename = f\"{os.path.splitext(os.path.basename(audio_path))[0]}_transcript.json\"\n",
    "    transcript_path = os.path.join(os.path.dirname(audio_path), new_filename)\n",
    "\n",
    "    # Save full API response as a new JSON file\n",
    "    with open(transcript_path, \"w\") as f:\n",
    "        json.dump(transcription.model_dump(), f, indent=2)\n",
    "\n",
    "    return transcript_path\n",
    "\n",
    "# Register as a StructuredTool\n",
    "transcribe_audio_tool = StructuredTool(\n",
    "    name=\"TranscribeAudio\",\n",
    "    func=transcribe_audio,\n",
    "    description=\"Transcribes an audio file into text using OpenAI Whisper API and saves the transcript with timestamps in a new file.\",\n",
    "    args_schema=TranscribeAudioSchema\n",
    ")\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "\n",
    "class CleanTranscriptSchema(BaseModel):\n",
    "    transcript_path: str\n",
    "\n",
    "def clean_transcript(transcript_path: str) -> str:\n",
    "    \"\"\"Cleans up the transcript while preserving word timestamps and inserting placeholders for removed filler words.\"\"\"\n",
    "\n",
    "    # Ensure file exists\n",
    "    if not os.path.exists(transcript_path):\n",
    "        raise FileNotFoundError(f\"Transcript file not found: {transcript_path}\")\n",
    "\n",
    "    # Load the original transcript JSON\n",
    "    with open(transcript_path, \"r\") as file:\n",
    "        transcript_data = json.load(file)\n",
    "\n",
    "    if \"words\" not in transcript_data:\n",
    "        raise ValueError(\"Invalid transcript format: Missing 'words' field.\")\n",
    "\n",
    "    words = transcript_data[\"words\"]\n",
    "    original_text = \" \".join([w[\"word\"] for w in words])\n",
    "    print(f\"‚úÖ Transcript passed to LLM for cleaning: {original_text}\")\n",
    "\n",
    "    # Update prompt to insert placeholders where filler words were removed\n",
    "    prompt = f\"\"\"\n",
    "    Clean up this transcript while preserving its structure:\n",
    "\n",
    "    - Remove filler words (e.g., \"uh\", \"um\", \"mm-hmm\").\n",
    "    - Wherever a filler word is removed, insert \"......\" as a placeholder.\n",
    "    - Improve readability and coherence.\n",
    "    - Do NOT modify word timestamps.\n",
    "    - Output only the cleaned words and placeholders.\n",
    "\n",
    "    Transcript:\n",
    "    {original_text}\n",
    "    \"\"\"\n",
    "\n",
    "    response = llm.invoke(prompt)\n",
    "    cleaned_text = response.content.strip()\n",
    "\n",
    "    # Split cleaned text into words (including placeholders)\n",
    "    cleaned_words = cleaned_text.split()\n",
    "    cleaned_word_list = []\n",
    "    original_word_iter = iter(words)\n",
    "\n",
    "    for cleaned_word in cleaned_words:\n",
    "        while True:\n",
    "            original_word = next(original_word_iter, None)\n",
    "            if original_word is None:\n",
    "                break  # No more words to process\n",
    "\n",
    "            if cleaned_word == \"......\":\n",
    "                # Insert a placeholder with no timestamp\n",
    "                cleaned_word_list.append({\"word\": \"......\"})\n",
    "                break\n",
    "\n",
    "            if original_word[\"word\"].lower() == cleaned_word.lower():\n",
    "                cleaned_word_list.append({\n",
    "                    \"word\": original_word[\"word\"],\n",
    "                    \"start\": original_word[\"start\"],\n",
    "                    \"end\": original_word[\"end\"]\n",
    "                })\n",
    "                break  # Move to the next cleaned word\n",
    "\n",
    "    # Save cleaned transcript with timestamps\n",
    "    cleaned_transcript_path = transcript_path.replace(\"_transcript.json\", \"_cleaned_transcript.json\")\n",
    "\n",
    "    cleaned_transcript_data = {\n",
    "        \"duration\": transcript_data.get(\"duration\"),\n",
    "        \"language\": transcript_data.get(\"language\"),\n",
    "        \"text\": \" \".join([w[\"word\"] for w in cleaned_word_list]),\n",
    "        \"words\": cleaned_word_list\n",
    "    }\n",
    "\n",
    "    with open(cleaned_transcript_path, \"w\") as f:\n",
    "        json.dump(cleaned_transcript_data, f, indent=2)\n",
    "\n",
    "    return cleaned_transcript_path\n",
    "\n",
    "\n",
    "# Register as a StructuredTool\n",
    "clean_transcript_tool = StructuredTool(\n",
    "    name=\"CleanTranscript\",\n",
    "    func=clean_transcript,\n",
    "    description=\"Cleans up a transcript using GPT-4o while preserving timestamps for the remaining words and adding pauses where words were removed.\",\n",
    "    args_schema=CleanTranscriptSchema\n",
    ")\n",
    "\n",
    "\n",
    "class GenerateAIVoiceSchema(BaseModel):\n",
    "    cleaned_transcript_path: str\n",
    "    output_audio_path: str = None\n",
    "    slow_down: bool = True  # Option to slow down the audio\n",
    "\n",
    "def generate_ai_voice_elevenlabs(\n",
    "    cleaned_transcript_path: str, \n",
    "    output_audio_path: str = None,\n",
    "    slow_down: bool = True\n",
    "    ) -> str:\n",
    "    \n",
    "    \"\"\"Generates AI voice from the cleaned transcript JSON using ElevenLabs.\"\"\"\n",
    "    ELEVENLABS_API_KEY = os.getenv(\"ELEVENLABS_API_KEY\")  # Load API key from env\n",
    "\n",
    "    if not ELEVENLABS_API_KEY:\n",
    "        raise ValueError(\"ElevenLabs API key is missing. Set it in the .env file.\")\n",
    "\n",
    "    # Load cleaned transcript JSON\n",
    "    with open(cleaned_transcript_path, \"r\") as file:\n",
    "        transcript_data = json.load(file)\n",
    "\n",
    "    # Ensure \"text\" field exists\n",
    "    if \"text\" not in transcript_data:\n",
    "        raise ValueError(\"Invalid cleaned transcript: Missing 'text' field.\")\n",
    "\n",
    "    text = transcript_data[\"text\"]  # Extract cleaned text\n",
    "    print(f\"‚úÖ Transcript passed to ElevenLabs: {text}\")\n",
    "\n",
    "    # Ensure the output path is set and within the same folder as the cleaned transcript\n",
    "    if output_audio_path is None:\n",
    "        filename = os.path.basename(cleaned_transcript_path).replace(\"_cleaned_transcript.json\", \"_ai.wav\")\n",
    "        output_audio_path = os.path.join(os.path.dirname(cleaned_transcript_path), filename)\n",
    "\n",
    "    # ElevenLabs API settings\n",
    "    voice_id = \"pMsXgVXv3BLzUgSXRplE\"  # ElevenLabs VOICE ID !!!! You must provide your desired voice ID here !!!!\n",
    "    url = f\"https://api.elevenlabs.io/v1/text-to-speech/{voice_id}\"\n",
    "\n",
    "    headers = {\n",
    "        \"xi-api-key\": ELEVENLABS_API_KEY,\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"text\": text,\n",
    "        \"model_id\": \"eleven_multilingual_v2\", # eleven_multilingual_v2, eleven_mlingual_v1\n",
    "        \"voice_settings\": {\n",
    "            \"stability\": 0.9,\n",
    "            \"similarity_boost\": 0.9\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Make API request\n",
    "    response = requests.post(url, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Raise error if request fails\n",
    "\n",
    "    # Save the generated AI audio file\n",
    "    with open(output_audio_path, \"wb\") as audio_file:\n",
    "        audio_file.write(response.content)\n",
    "\n",
    "    print(f\"‚úÖ AI voice successfully generated: {output_audio_path}\")\n",
    "\n",
    "    # If slow_down is enabled, override the file with slowed-down audio\n",
    "    if slow_down:\n",
    "        print(f\"‚úÖ Slowing down the audio!\")\n",
    "        slow_down_audio(output_audio_path)\n",
    "\n",
    "    return output_audio_path\n",
    "    \n",
    "\n",
    "def slow_down_audio(audio_path: str, slowdown_factor: float = 0.90):\n",
    "    \"\"\"Slows down the AI-generated voice recording while preserving pitch.\"\"\"\n",
    "    if not os.path.exists(audio_path):\n",
    "        raise FileNotFoundError(f\"Audio file not found: {audio_path}\")\n",
    "\n",
    "    temp_output_path = audio_path.replace(\".wav\", \"_temp.wav\")\n",
    "\n",
    "    # Construct FFmpeg command\n",
    "    command = [\n",
    "        \"ffmpeg\", \"-i\", audio_path,\n",
    "        \"-filter:a\", f\"atempo={slowdown_factor}\",\n",
    "        \"-y\", temp_output_path\n",
    "    ]\n",
    "\n",
    "    # Execute FFmpeg\n",
    "    subprocess.run(command, check=True)\n",
    "\n",
    "    # Replace original file with slowed-down version\n",
    "    os.replace(temp_output_path, audio_path)\n",
    "    print(f\"‚úÖ AI voice successfully slowed down: {audio_path}\")\n",
    "\n",
    "# Register as a StructuredTool\n",
    "ai_voice_tool = StructuredTool(\n",
    "    name=\"GenerateAIVoice\",\n",
    "    func=generate_ai_voice_elevenlabs,\n",
    "    description=\"Generates AI voice from the cleaned transcript using ElevenLabs, with optional slowdown feature.\",\n",
    "    args_schema=GenerateAIVoiceSchema\n",
    ")\n",
    "\n",
    "\n",
    "def remove_old_voice(video_path: str, output_video_path: str = None) -> str:\n",
    "    \"\"\"Removes all audio from the original video using FFmpeg.\"\"\"\n",
    "    \n",
    "    # Ensure the function does not create endless `_cleaned.mov` files\n",
    "    if \"_cleaned\" in video_path:\n",
    "        print(f\"‚ö†Ô∏è Skipping already cleaned video: {video_path}\")\n",
    "        return video_path  # Return the same path if it's already cleaned\n",
    "\n",
    "    if output_video_path is None:\n",
    "        base_name = os.path.splitext(video_path)[0]\n",
    "        output_video_path = f\"{base_name}_cleaned.mov\"\n",
    "\n",
    "    print(f\"üßπ Removing all audio from {video_path} -> {output_video_path}\")\n",
    "\n",
    "    # FFmpeg command to remove all audio\n",
    "    result = subprocess.run([\n",
    "        \"ffmpeg\", \"-i\", video_path, \"-c:v\", \"copy\", \"-an\", \"-y\", output_video_path\n",
    "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ùå FFmpeg error while removing audio: {result.stderr}\")\n",
    "        return None\n",
    "\n",
    "    if not os.path.exists(output_video_path):\n",
    "        print(f\"‚ùå Error: Cleaned video {output_video_path} was not created.\")\n",
    "        return None\n",
    "\n",
    "    print(f\"‚úÖ Successfully created cleaned video: {output_video_path}\")\n",
    "    return output_video_path\n",
    "\n",
    "\n",
    "# Register the tool in LangGraph\n",
    "remove_old_voice_tool = Tool(\n",
    "    name=\"RemoveOldVoice\",\n",
    "    func=remove_old_voice,\n",
    "    description=\"Removes all audio from a video file using FFmpeg, creating a silent version.\"\n",
    ")\n",
    "\n",
    "\n",
    "def merge_audio(video_path: str, ai_voice_path: str, output_video_path: str = None):\n",
    "    \"\"\"\n",
    "    Merges the new AI-generated voice into a cleaned version of the video,\n",
    "    WITHOUT applying any timing adjustments to the AI voice.\n",
    "    If the passed video_path does not end with \"_cleaned.mov\", it appends \"_cleaned.mov\".\n",
    "    \"\"\"\n",
    "\n",
    "    # If user passes e.g. \"myvideo_cleaned.mov\", use that directly.\n",
    "    # Otherwise, assume the cleaned version is \"myvideo_cleaned.mov\".\n",
    "    if video_path.endswith(\"_cleaned.mov\"):\n",
    "        cleaned_video_path = video_path\n",
    "        # Remove \"_cleaned\" to get the base for naming final output if needed\n",
    "        base_name = os.path.splitext(video_path)[0].replace(\"_cleaned\", \"\")\n",
    "    else:\n",
    "        base_name = os.path.splitext(video_path)[0]\n",
    "        cleaned_video_path = f\"{base_name}_cleaned.mov\"\n",
    "\n",
    "    # Ensure the cleaned video exists\n",
    "    if not os.path.exists(cleaned_video_path):\n",
    "        print(f\"‚ùå Error: Cleaned video file {cleaned_video_path} does not exist.\")\n",
    "        return None\n",
    "\n",
    "    # If user didn't supply output, create it as \"myvideo_final.mov\" or similar\n",
    "    if output_video_path is None:\n",
    "        output_video_path = f\"{base_name}_final.mov\"\n",
    "\n",
    "    print(f\"üé¨ Merging (no timing adjust) video: {cleaned_video_path} + AI voice: {ai_voice_path}\")\n",
    "\n",
    "    # FFmpeg command: Replace the cleaned video audio track with the AI-generated voice\n",
    "    result = subprocess.run([\n",
    "        \"ffmpeg\", \"-i\", cleaned_video_path, \"-i\", ai_voice_path,\n",
    "        \"-c:v\", \"copy\", \"-c:a\", \"aac\",\n",
    "        \"-map\", \"0:v:0\", \"-map\", \"1:a:0\",\n",
    "        \"-movflags\", \"faststart\", \"-y\", output_video_path\n",
    "    ], stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
    "\n",
    "    if result.returncode != 0:\n",
    "        print(f\"‚ùå FFmpeg error while merging audio and video: {result.stderr}\")\n",
    "        return None\n",
    "\n",
    "    print(f\"‚úÖ Successfully created final video: {output_video_path}\")\n",
    "    return output_video_path\n",
    "\n",
    "\n",
    "# Registering MergeAudio as a StructuredTool\n",
    "class MergeAudioSchema(BaseModel):\n",
    "    video_path: str\n",
    "    ai_voice_path: str\n",
    "    output_video_path: str = None  # Optional parameter\n",
    "\n",
    "merge_audio_tool = StructuredTool(\n",
    "    name=\"MergeAudio\",\n",
    "    func=merge_audio,\n",
    "    description=\"Merges the new AI-generated voice into the original .mov video using FFmpeg.\",\n",
    "    args_schema=MergeAudioSchema\n",
    ")\n",
    "\n",
    "# Tools for LangGraph\n",
    "tools = [\n",
    "    extract_audio_tool,\n",
    "    transcribe_audio_tool,\n",
    "    clean_transcript_tool,\n",
    "    ai_voice_tool,\n",
    "    remove_old_voice_tool,\n",
    "    merge_audio_tool\n",
    "]\n",
    "\n",
    "# System Prompt for LangGraph\n",
    "system_prompt = \"\"\"You are an AI assistant that processes videos by replacing their original voice with an AI-generated voice. \n",
    "Your workflow includes: \n",
    "extracting audio from the video, \n",
    "transcribing it with timestamps, \n",
    "cleaning the transcript to remove filler words, \n",
    "generating a new AI voice from the cleaned text, \n",
    "and merging it back into the video while removing the old voice. \n",
    "Ensure high accuracy and natural-sounding output.\n",
    "\n",
    "**IMPORTANT:**  \n",
    "- `RemoveOldVoice` **must always run before** `MergeAudio`.  \n",
    "- `TranscribeAudio` should only run **after** `ExtractAudio` has successfully completed.  \n",
    "- Do not attempt to merge the AI voice until `RemoveOldVoice` has successfully produced a cleaned video.  \n",
    "\"\"\"\n",
    "\n",
    "# Create LangGraph Agent\n",
    "graph = create_react_agent(\n",
    "    model=ChatOpenAI(model=\"gpt-4o\"),\n",
    "    tools=tools,\n",
    "    prompt=system_prompt,\n",
    "    debug=False\n",
    ")\n",
    "\n",
    "# Process Video Function\n",
    "def process_video(video_path):\n",
    "    \"\"\"Runs the complete AI workflow for voice replacement.\"\"\"\n",
    "    \n",
    "    inputs = {\"messages\": [(\"user\", f\"Help me record a clean voice over for my {video_path} video and make sure to merge the clean AI voice back in the video.\")]}\n",
    "    \n",
    "    for step in graph.stream(inputs, stream_mode=\"steps\"):\n",
    "        message = step[\"messages\"][-1]\n",
    "        if isinstance(message, tuple):\n",
    "            print(message)\n",
    "        else:\n",
    "            message.pretty_print()\n",
    "\n",
    "# Example Usage\n",
    "process_video(\"downloads/CleanVoice.mov\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098d2f67-2913-4bd2-9efc-0e9e4a844f82",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440cd5eb-7b35-4998-90de-49e6fd464e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
